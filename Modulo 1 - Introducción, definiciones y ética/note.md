
- Modulo I - Introducci√≥n, definiciones y √©tica
    - [Clase 1 Introducci√≥n y definiciones](#clase-1-introducci√≥n-y-definiciones)
    - [Clase 2 √âtica y Legalidad](#√©tica-y-legalidad)
    - [Clase 3 Configuraci√≥n del entorno de trabajo con Jupyter](#clase-3-configuraci√≥n-del-entorno-de-trabajo-con-jupyter)




# 1. **Introducci√≥n y definiciones**

`Web Scraping:` Proceso de extracci√≥n de datos almacenados en la web.

Objetivo: Recopilar informaci√≥n almacenada en un servicio web.

Aplicaci√≥n: Productos, rese√±as, noticias para hacer posteriormente un an√°lisis.

---

`Web Crawling`: Proceso de mapeo e indexaci√≥n de p√°ginas web para conocer su contenido

Objetivo: Conocer la estructura de la web.

Aplicaci√≥n: Conocer la estructura de la web con el fin de indexar en motores de b√∫squeda.


**Herramientas del curso:**

- Python
- Jupyter
- Bibliotecas
    - Requests
    - BeautifulSoup
    - Selenium
    - Scrapy

**Lecturas recomendadas:**
- [Documentacion Requets](https://requests.readthedocs.io/en/latest/)
- [Documentacion Beautifulsoup ](https://www.crummy.com/software/BeautifulSoup/)
- [Documentacion Selenium](https://selenium-python.readthedocs.io/)
- [Documentacion Scrapy](https://scrapy.org/)

# 2. **√âtica y Legalidad**

## **¬øEs legal?**
Debemos hacernos las siguientes preguntas.
>¬øEstoy violando alguna reglamentaci√≥n local?
¬øEstoy violando los ‚ÄúT√©rminos y Condiciones‚Äù del sitio?
¬øEstoy accediendo a lugares no autorizados?
¬øEs legal el uso que le voy a dar a los datos?
> 

Tenemos estos articulos que hablen y responden estas preguntas.

[https://www.forbes.com/sites/emmawoollacott/2019/09/10/linkedin-data-scraping-ruled-legal/#66d703ba1b54](https://www.forbes.com/sites/emmawoollacott/2019/09/10/linkedin-data-scraping-ruled-legal/#66d703ba1b54)

[https://nubela.co/blog/is-linkedin-scraping-legal/](https://nubela.co/blog/is-linkedin-scraping-legal/)


---

`Robots.txt` En este archivo encontramos informaci√≥n sobre el sitio y nos muestra que  sitios o rutas el due√±o de la p√°gina no quiere que accedamos. 

Casi todas las paginas tienen un archivo robots.txt para darnos  las buenas practicas de obtenci√≥n de esas paginas, pero no es  vinculante y no hay nada que no nos permita escrapear todo el sitio. Solo se responsable.

`Disallow:` Rutas que no quieren que se le haga scraping o que sean indexadas. 

`Crawl-delay`: Es el tiempo que debemos configuraci√≥n entre solicitud al sitio. Para no sobrecargar este sitio.

üí° Importante tener en cuenta este valor:

```
crawl-delay: 30
```
expresado en segundos, y usado en la colecci√≥n de par√°metros a la hora de hacer de sistematizar consultas (scraping) a un sitio web respetando la integridad de respuesta del servidor objetivo