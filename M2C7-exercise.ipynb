{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unificando el scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_principal='https://www.pagina12.com.ar'\n",
    "r_web_principal=requests.get(link_principal)\n",
    "s_web_principal = BeautifulSoup(r_web_principal.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seleccionamos el primer elemento de la lista de secciones\n",
    "secciones_web_principal = s_web_principal.find_all('div', attrs={'class':'p12-dropdown-column'})[0]\n",
    "link_secciones_web_principal = secciones_web_principal.find_all('a')\n",
    "#recorremos la lista de tag a\n",
    "URL_secciones=[]\n",
    "for i in link_secciones_web_principal:\n",
    "    if len(link_secciones_web_principal):\n",
    "        URL_secciones.append(i.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_URL_secciones= requests.get(URL_secciones[0])\n",
    "s_seccion= BeautifulSoup(r_URL_secciones.text, 'lxml')\n",
    "def obtener_noticias(soup):\n",
    "    lista_notas=[]\n",
    "    #obtener noticias promocionadas\n",
    "    noticias_promocionadas=soup.find('div', attrs={'class':'article-item__content'})\n",
    "    if noticias_promocionadas:\n",
    "        lista_notas.append(f'{link_principal}{noticias_promocionadas.a.get(\"href\")}')\n",
    "\n",
    "    #resto de noticias\n",
    "    resto_noticias=soup.find('section', attrs={'class':'list-content'})\n",
    "    for i in resto_noticias.find_all('div', attrs={'class':'article-item__header'}):\n",
    "        if i.a:\n",
    "            lista_notas.append(f'{link_principal}{i.a.get(\"href\")}')\n",
    "\n",
    "    return lista_notas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_notas=(obtener_noticias(s_seccion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtemos informacion de cada nota\n",
    "# seleccionamos la primera nota de nuestra lista de notas\n",
    "URL_nota=lista_notas[0]\n",
    "r_URL_nota= requests.get(URL_nota)\n",
    "s_nota=BeautifulSoup(r_URL_nota.text, 'lxml')\n",
    "\n",
    "def obtener_info(s_nota):\n",
    "    #creamos un diccionario vacio para guardar informacion de la notas\n",
    "    info_dic={}\n",
    "\n",
    "    #extraemos el titulo de la nota\n",
    "    titulo = s_nota.find('h1').text\n",
    "    if titulo:\n",
    "        info_dic['titulo']=titulo\n",
    "    else:\n",
    "        info_dic['titulo']=None\n",
    "\n",
    "    #extraemos la fecha de la nota\n",
    "    fecha = s_nota.find('time').get_text()\n",
    "    if fecha:\n",
    "        info_dic['fecha']=fecha\n",
    "    else:\n",
    "        info_dic['fecha']=None\n",
    "\n",
    "    #extraemos el copete\n",
    "    copete= s_nota.find('h4').get_text()\n",
    "    if copete:\n",
    "        info_dic['copete']=copete\n",
    "    else:\n",
    "        info_dic['copete']=None\n",
    "\n",
    "    #extraemos la volanta\n",
    "    volanta= s_nota.find('h3').get_text()\n",
    "    if volanta:\n",
    "        info_dic['volanta']=volanta\n",
    "    else:\n",
    "        info_dic['volanta']=None\n",
    "\n",
    "    #extraemos el cuerpo de la nota\n",
    "    cuerpo= s_nota.find('div', attrs={'class':'article-main-content article-text'}).find_all('p')\n",
    "    cu=[]\n",
    "    for i in cuerpo:\n",
    "        cu.append(i.get_text())\n",
    "    \n",
    "    #utilizamos el metodo join, para convertir una lista str a una cadena de texto\n",
    "    if cu:\n",
    "        info_dic['cuerpo']=\"\".join(map(str, cu))\n",
    "    else:\n",
    "        info_dic['cuerpo']=None\n",
    "    \n",
    "    #extraemos el autor de la nota\n",
    "    autor= s_nota.find('div', attrs={'class':'author-name'})\n",
    "    if autor:\n",
    "        info_dic['autor']=autor.get_text()\n",
    "    else:\n",
    "        info_dic['autor']=None\n",
    "    \n",
    "    #extraemos la imagen\n",
    "    #parseamos el tag div que contiene la imagen de la nota\n",
    "    media = s_nota.find('div', attrs={'class':'article-main-media-image__container'})\n",
    "    if len(media):\n",
    "        imagen_src=media.img.get('src')\n",
    "        try:\n",
    "            r_img=requests.get(imagen_src)\n",
    "            if r_img.status_code== 200:\n",
    "                info_dic['imagen']=r_img.content\n",
    "            else:\n",
    "                info_dic['imagen']=None\n",
    "        except:\n",
    "            print('No se pudo obtener la imagen')\n",
    "    else:\n",
    "        print ('No se encontraron imagenes')\n",
    "\n",
    "    return  info_dic\n",
    "#obtiene todos los valores de un diccionario\n",
    "#print (dic.values())\n",
    "#obtener un valor de un diccionario, utilizamos la libreria de imagen de python para mostrar la img\n",
    "#Image(dic.get('imagen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos nuestra funcion para recorrer cada noticia de las secciones\n",
    "# de esta manera poder obtener la informacion de cada noticia\n",
    "# y devolvemos un diccionario de todos los datos de la noticia\n",
    "def scrape_nota(url):\n",
    "    try: \n",
    "        r_nota=requests.get(url)\n",
    "    except Exception as e:\n",
    "        print ('Error scrapeando URL ->', url)\n",
    "        print (e)\n",
    "        return None\n",
    "    \n",
    "    if r_nota.status_code!= 200:\n",
    "        print(f'Error obteniendo nota {url}')\n",
    "        print(f'Status Code = {r_nota.status_code}')\n",
    "        return None\n",
    "    \n",
    "    s_nota=BeautifulSoup(r_nota.text, 'lxml')\n",
    "\n",
    "    dic_scrape=obtener_info(s_nota)\n",
    "    dic_scrape['url']=url\n",
    "\n",
    "    return dic_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_secciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se pudo obtener la seccion -> https://www.pagina12.com.ar/suplementos/cultura-y-espectaculos\n",
      "No se pudo obtener la seccion -> https://www.pagina12.com.ar/edicion-impresa\n"
     ]
    }
   ],
   "source": [
    "#obtenemos todos los link de la primera pagina de cada seccion \n",
    "notas_secciones=[]\n",
    "for link in URL_secciones:\n",
    "    try:\n",
    "        r_2_URL_secciones=requests.get(link)\n",
    "        if r_2_URL_secciones.status_code== 200:\n",
    "            #parseamos cada seccion \n",
    "            s_2_web_principal=BeautifulSoup(r_2_URL_secciones.text, 'lxml')\n",
    "            # enviamos el link de la seccion a nuestra funcion obtener notas \n",
    "            # y guaramos lo que retorna la funcion (diccionario) a nuestra lista de notas de secciones\n",
    "            notas_secciones.extend(obtener_noticias(s_2_web_principal))\n",
    "        else:\n",
    "            print ('No se pudo obtener la seccion ->', link)\n",
    "    except:\n",
    "        print ('No se pudo obtener la seccion ->', link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(notas_secciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapeando nota 0 / 86\n",
      "Scrapeando nota 1 / 60\n",
      "Scrapeando nota 2 / 95\n",
      "Scrapeando nota 3 / 89\n",
      "Scrapeando nota 4 / 65\n",
      "Scrapeando nota 5 / 95\n",
      "Scrapeando nota 6 / 73\n",
      "Scrapeando nota 7 / 80\n",
      "Scrapeando nota 8 / 76\n",
      "Scrapeando nota 9 / 95\n",
      "Scrapeando nota 10 / 95\n",
      "Scrapeando nota 11 / 84\n"
     ]
    }
   ],
   "source": [
    "# creamos nuestro data set de noticias de cada seccion\n",
    "data_set=[]\n",
    "# recorremos cada noticia de cada seccion\n",
    "# en cada noticia utilizamos el metodo scrape_nota que a su vez llama a obtener_info\n",
    "# con el resultado de obtener_info, vamos guardando en un diccionario -> data_set\n",
    "for posicion_nota,link_nota in enumerate(notas_secciones):\n",
    "    if posicion_nota<12:\n",
    "        print (f'Scrapeando nota {posicion_nota} / {len(link_nota)}')\n",
    "        data_set.append(scrape_nota(link_nota))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos nuestro data frame utilizando pandas data frame\n",
    "data_frame= pd.DataFrame(data_set)\n",
    "#podemos visualizar con las cabeceras\n",
    "data_frame.head()\n",
    "# pasamos nuestro conjunto de datos a un archivo csv\n",
    "data_frame.to_csv('Notas pagina12.csv')\n",
    "# pasamos nuestro conjunto de datos a un archivo excel\n",
    "data_frame.to_excel(\"Notas Paginas12.xlsx\", sheet_name=\"data frame\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('entv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8491acde47aa26015fdf488fd68628541edd72931848a0305ddc7dd62f03120a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
