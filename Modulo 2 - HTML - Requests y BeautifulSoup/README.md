# Curso de Web Scraping: ExtracciÃ³n de Datos en la Web

## **Modulo II - HTML: Requests y BeautifulSoup**
- [Clase 4 Descargando una pÃ¡gina web](#4-descargando-una-pÃ¡gina-web)



# 4. **Descargando una pÃ¡gina web**

## Parsing Pagina12

En este mÃ³dulo veremos cÃ³mo utilizar las bibliotecas `requests` y `bs4` para programar scrapers de sitios HTML es decir, sitios estaticos. Nos propondremos armar un scraper de noticias del diario [www.pagina12.com.ar](http://www.pagina12.com.ar/)

Supongamos que queremos leer el diario por internet. Lo primero que hacemos es abrir el navegador, escribir la URL del diario y apretar `Enter` para que aparezca la pÃ¡gina del diario. Lo que ocurre en el momento en el que apretamos `Enter` es lo siguiente:

1. El navegador envÃ­a una solicitud a la URL pidiÃ©ndole informaciÃ³n.
2. El servidor recibe la peticiÃ³n y procesa la respuesta.
3. El servidor envÃ­a la respuesta a la IP de la cual recibiÃ³ la solicitud.
4. Nuestro navegador recibe la respuesta y la muestraÂ **formateada**Â en pantalla.

Para hacer un scraper debemos hacer un programa que replique este flujo de forma automÃ¡tica y sistemÃ¡tica para luego extraer la informaciÃ³n deseada de la respuesta. 

UtilizaremosÂ `requests`Â para realizar peticiones y recibir las respuestas yÂ `bs4`Â paraÂ *parsear*Â la respuesta y extraer la informaciÃ³n.

Te dejo unos links que tal vez te sean de utilidad:

- [CÃ³digos de status HTTP](https://developer.mozilla.org/es/docs/Web/HTTP/Status)
- [DocumentaciÃ³n de requests](https://requests.kennethreitz.org/en/master/)
- [DocumentaciÃ³n de bs4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

Antes de seguir necesitamos instalar las siguientes librerÃ­as:

- jupyter
- requests
- bs4


> ðŸ’¡ iniciamos nuestro entorno virtual, e instalamos.


```python
# importamos la biblioteca requests
import requests

#guardamos la URL en una variable
url='https://www.pagina12.com.ar/'

# generamos una solicitud a nuestra URL y guardamos el resultado
p12 = requests.get(url)

# verificamos si salio todo bien
p12.status_code
    # Respuestas informativas (100â€“199),
    # Respuestas satisfactorias (200â€“299),
    # Redirecciones (300â€“399),
    # Errores de los clientes (400â€“499),
    # y errores de los servidores (500â€“599).

# ver el contenido
print(p12.text)

#Buscar un tÃ­tulo en el texto
#Muchas veces la respuesta a la solicitud puede ser algo que no sea un texto: una imagen, un archivo de audio, un video, etc.
p12.content

#Analicemos otros elementos de la respuesta. Encabezados de la respuesta
#vamos a ver el encabezado de la respuesta
p12.headers

#encabezado de la solicitud que hacemos
# existen paginas webs que al detectar el user-agent con nombre por defecto, bloquean la respuesta, solucion enmascarar el nombre
p12.request.headers

#que metodos utilizamos, existen varios metodos PUT, DELETE etc 
#El contenido de la request que acabamos de hacer estÃ¡ avisando que estamos utilizando la biblioteca requests para python 
#y que no es un navegador convencional. Se puede modificar
p12.request.method

#volver a consultar la URL a quien hicimos la solicitud
# util en caso de que existan redireccionamiento (cuando accedemos a un sitio y este nos redirecciona a otro sitio)
p12.request.url

```

## Resumen

- p12 = requests.get(url) // realiza la solicitud.
- p12.status_code // muestra el cÃ³digo de status HTTP de la respuesta.
- print(p12.text) // muestra el HTML de la pÃ¡gina sin formatear.
- p12.content // muestra el contenido de la respuesta, puede ser bytes, imÃ¡genes, audio.
- p12.headers // muestra el encabezado de la respuesta.
- p12.request.headers // muestra el encabezado de la solicitud. El contenido de esta request avisa al servidor que se estÃ¡ utilizando requests en python y que no es un navegador convencional. Puede ser modificado.
- p12-request.url // muestra la url a la que se le hizo la solicitud.